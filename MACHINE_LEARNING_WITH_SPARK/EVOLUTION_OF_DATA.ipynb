{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a785ca7a",
   "metadata": {},
   "source": [
    "# EVOLUTION OF DATA\n",
    "\n",
    "<br/>Hernan Carlos Chavez Paura Garcia\n",
    "<br/>Jul 14th, 2023\n",
    "<br/>* Singh, Pramod. Machine Learning with PySpark: With Natural Language Processing and Recommender Systems. Apress, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79062e07",
   "metadata": {},
   "source": [
    "## DATA GENERATION\n",
    "\n",
    "<br/><br/><center>Fig. 1 Data Evolution</center>\n",
    "\n",
    "<center><img src=\"IMAGES/Fig_1.png\" width=\"900\"></center><br/><br/>\n",
    "\n",
    "<br/><br/><center>Fig. 2 Single Thread Processing</center>\n",
    "\n",
    "<center><img src=\"IMAGES/Fig_2.png\" width=\"600\"></center><br/><br/>\n",
    "\n",
    "<br/><br/><center>Fig. 3 Parallel Processing</center>\n",
    "\n",
    "<center><img src=\"IMAGES/Fig_3.png\" width=\"600\"></center><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c015e1b2",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "<br/><br/><center>Fig. 4 Spark Evolution</center>\n",
    "\n",
    "<center><img src=\"IMAGES/Fig_4.png\" width=\"600\"></center><br/><br/>\n",
    "\n",
    "Under the hood, Spark uses a different data structure known as RDD (Resilient Distributed Datasets). It is resilient in a sense that they have ability to re-create any point of time during the execution process. So RDD creates a new RDD using the last one and always has the ability to reconstruct in case of any error. They are also immutable as original RDDs remain unaltered. As Spark is a distributed framework, it works on master and worker node settings as shown in Figure 5. The code to execute any of the activities is first written on Spark Driver, and that is shared across worker nodes where the data actually resides. Each worker node containes Executors that will actually execute the code. Cluster Manager keeps a check on the availability of various worker nodes for the next task allocation.\n",
    "\n",
    "<br/><br/><center>Fig. 5 Spark Functioning</center>\n",
    "\n",
    "<center><img src=\"IMAGES/Fig_5.png\" width=\"600\"></center><br/><br/>\n",
    "\n",
    "Since Spark is a generic data processing engine, it can easily be used with various data sources such as HBase, Cassandra, Amazon S3, HDFS, etc. Spark provides the users four langua options to use on it: java, Python, Scala, and R."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb98704",
   "metadata": {},
   "source": [
    "## Spark Core\n",
    "\n",
    "All the features on Spark are built on top of Spark Code. Spark Core is responsible for managing tasks, I/O operations, fault tolerance, and memory management, etc.\n",
    "\n",
    "<br/><center>Fig. 6 Spark Architecture</center>\n",
    "\n",
    "<center><img src=\"IMAGES/Fig_6.png\" width=\"600\"></center><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8f4e55",
   "metadata": {},
   "source": [
    "## Spark Components: Spark SQL\n",
    "\n",
    "This component mainly deals with structured data processing. The key idea is to fetch more information about the structure of the data to perform additional optimization. It can be considered a distributed SQL query engine.\n",
    "\n",
    "\n",
    "## Spark Components: Spark Streaming\n",
    "\n",
    "This component deals with processing the real-time streaming data in a scalable and fault tolerant manner. It uses micro batching to read and process incoming streams of data. It creates micro batches of streaming data, executes batch processing, and passes it to some file storage or live dashboard. Spark Streaming can ingest the data from multiple sources like Kafka and Flume.\n",
    "\n",
    "\n",
    "## Spark Components: Spark MLlib\n",
    "\n",
    "This componewnt is used for building Machine Learning Models on Big Data in a distributed manner. The traditional technique of building ML models using Python's scikit learn library faces a lot of challenges when data size is huge whereas MLlib is designed in a way that offers feature engineering and machine learning at scale. MLlib has most of the algorithms implemented for classification, regression, clustering, recommendation system, and natural langua processing.\n",
    "\n",
    "\n",
    "## Spark Components: Spark GraphX/Graphframe\n",
    "\n",
    "This compoenent excels in graph analytics and graph parallel execution. Graph frames can be used to understand the underlying relationships and visualize the insights from data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddc40ce",
   "metadata": {},
   "source": [
    "## Docker\n",
    "\n",
    "We can directly use PySpark with Docker using an image from the repository of Jupyter but that requires Docker installed on your system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68baec70",
   "metadata": {},
   "source": [
    "## Databricks\n",
    "\n",
    "Databricks also offers a community edition account that is free of cost and provides 6 GB clusters with PySpark."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
